#Do not change the way of queries, sparkProperties, sparkSqlProperties. Just change the values inside strings

#queries to be run
queries="\"1-2-3-4-5-6-7-8-9-10-11-12-13-14-15-16-17-18-19-20-22\""
#queries=10
#queries="\"1-3-14\""


#spark Properties specified in lead
#spark-executor-cores has nothing to do with CPU cores available.
#sparkProperties="-J-Xmx2g -spark.network.timeout=300s -spark.driver.maxResultSize=2g -spark.shuffle.sort.bypassMergeThreshold=28"
sparkProperties="-heap-size=1g -memory-size=1g -conserve-sockets=false"

#Spark Sql properties are specified while executing query
#spark.sql.shuffle.partitions=${shufflePartitions},spark.sql.inMemoryColumnarStorage.compressed=${inMemoryColumnarStorageCompressed}
#spark.sql.inMemoryColumnarStorage.compressed=false
sparkSqlProperties="\"\""

#location of checkout
SnappyData=<SnappyData-build-path>
#SnappyData=/home/perfuser/SNAPPY/CHECKOUT/snappydata-0.7-bin

#number of buckets for order and lineitem tables
buckets_Order_Lineitem=128

#number of buckets for Customer, Part and PartSupplier tables
buckets_Cust_Part_PartSupp=128

#Are Nation, Region, Supplier tables column tables?
IsSupplierColumnTable=true

#number of buckets for Nation, Region, Supplier
buckets_Supplier=128

#Redundancy Level
Redundancy=0

#Whether region is persistent
Persistence=false

#Type of Persistence
Persistence_Type=SYNCHRONOUS

#Loading from parquet or csv file
Parquet=true

# Create Parquet files from CSV (tbl) files for future tests
# When Parquet=true, the table creation job will expect parquet files to already exist in order to load data into Snappy tables.
# When Parquet=false, one can create parquet files in table creation job for use in query execution if createParquet=true
createParquet=true

#From how many files data should be loaded in table
#NumberOfLoadStages=10
NumberOfLoadStages=1

#Machine Setup
locator=<host1>
leads=<host1>
servers=(<host2> <host3> <host4>)
#servers=(localhost)

#Port required while doing execution through JDBC
port=1527

#Log Directories
locatorDir=<locator-log-directory-path>
leadDir=<lead-log-directory-path>
serverDir=<server-log-directory-path>

#Locator Properties
locatorProperties="-conserve-sockets=false -J-Xloggc:gc.log -J-XX:+PrintGCDetails -J-XX:+PrintGCDateStamps -J-XX:+PrintGCTimeStamps -J-XX:+PrintHeapAtGC"

#Server properties
#serverMemory="-heap-size=1g -memory-size=2g  -conserve-sockets=false-J-XX:NewRatio=9 -J-Xloggc:gc.log -J-XX:+PrintGCDetails -J-XX:+PrintGCDateStamps -J-XX:+PrintGCTimeStamps -J-XX:+PrintHeapAtGC"
serverMemory="-heap-size=1g -memory-size=2g"

### While collecting results for result validation, always keep isDynamic=false and resultCollection=false. In this case WarmupRuns and AverageRuns will not be considered
### In case of Performance run, isDynamic is true, resultCollection is false and set the required number of warmupRuns and AverageRuns
#represent whether query should use dynamic parameters or static
IsDynamic=true

#Whether to collect results.For performance testing this should be false.
ResultCollection=true

#warmUpIterations
WarmupRuns=2
#Actual runs whose average will be taken and reported as performance
AverageRuns=3

# location of jar which has TPCH related class files
TPCHJar=<TPCHJar-path>

#Size of the TPCH data. Do not change format
#dataSize=1GB_Stages
dataSize=1GB

#Location of the TPCH Data. Make sure directory name is same as the dataSize specified above
dataDir=<data-directory-path>
#dataDir=/home/perfuser/SNAPPY/DATA/TPCH/$dataSize

#Location where final output will be collected
outputLocation=<path-to-location-for-generating-results>

#GoldenFile against which query results should be compared
ExpectedResultsAvailableAt=<path-to-stock-spark-result>
#ExpectedResultsAvailableAt=/home/perfuser/SNAPPY/CURRENTWORK/volumeTesting/STOCK_SPARK_RESULT

#Location of query results files
ActualResultsAvailableAt=<path-to-actual-results>
#ActualResultsAvailableAt=/home/perfuser/SNAPPY/CURRENTWORK/volumeTesting/23May_SNAPPY_RESULT

# Trace events to get granular timing info and report in loadPerfStream
traceEvents=true

# seed value for random number used in generating query parameter data
randomSeed=42

threadNumber=1